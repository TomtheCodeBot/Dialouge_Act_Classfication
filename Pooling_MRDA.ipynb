{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pooling_MRDA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9531dab51e2642839f1554b484e47976":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9b8354c252294f9d8653a1eb2ec9d3fe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a3a312d3156a4bd094474fa17c1a65fe","IPY_MODEL_99fb8c1ff81244f3858cbf7ef5cbcaaa","IPY_MODEL_a46cdc7b288941b3adb5807e931d059d"]}},"9b8354c252294f9d8653a1eb2ec9d3fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3a312d3156a4bd094474fa17c1a65fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_956cc5fbf44a4a6fab3bd40aba508c55","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c452e8c4388949d7a19190c4540e91a9"}},"99fb8c1ff81244f3858cbf7ef5cbcaaa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_db03e5467ce94359b15c670c540bb078","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97c5e0e474454608bb80e9c854110a8c"}},"a46cdc7b288941b3adb5807e931d059d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c1730b9373d343abb5a20abecc4d2d71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 420M/420M [00:09&lt;00:00, 47.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_efbe4c866d5a4bc39b3d40d936ad5e4c"}},"956cc5fbf44a4a6fab3bd40aba508c55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c452e8c4388949d7a19190c4540e91a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db03e5467ce94359b15c670c540bb078":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97c5e0e474454608bb80e9c854110a8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1730b9373d343abb5a20abecc4d2d71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"efbe4c866d5a4bc39b3d40d936ad5e4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Pij3jwxUjMbT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45bf90a6-5022-40b2-af6b-028d4e89c055"},"source":["!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-48ff14f5-7cdb-91fe-80d7-6891f52c2fa3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"D80Oan9MlOaN"},"source":["## 1.Data"]},{"cell_type":"markdown","metadata":{"id":"ATw8NcN7ooEB"},"source":["### 1.1 Preprocessing"]},{"cell_type":"code","metadata":{"id":"t-J0Dmh0lVRZ"},"source":["import string\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOpKsTUMkCD7"},"source":["def seperatePunct(text):\n","    x = re.sub( r'([a-zA-Z])([,.!?])', r'\\1 \\2', text)\n","    return x\n","\n","def removeTag(text):\n","    x = re.sub(\"\\\\{.{1}([^}]+)\\\\}\", '\\\\1', text)\n","    return x\n","\n","def lower_text(text):\n","    return text.lower()\n","\n","def changeTag(text):\n","    x = re.sub( r'\\<([a-zA-Z]) ([a-zA-Z])\\>', r'\\1 \\2', text)\n","    return x\n","\n","def normalizeWhitespace(text):\n","    result = str(text)\n","    result = re.sub(r\"//t\",r\"\\t\", result)\n","    result = re.sub(r\"( )\\1+\",r\"\\1\", result)\n","    result = re.sub(r\"(\\n)\\1+\",r\"\\1\", result)\n","    result = re.sub(r\"(\\r)\\1+\",r\"\\1\", result)\n","    result = re.sub(r\"(\\t)\\1+\",r\"\\1\", result)\n","\n","    return result.strip(\" \")\n","\n","### Removing punctuations. (!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~)\n","def removePunctuations(text,exception=\"\"):\n","    remove=string.punctuation+\"“\"+\"”\"+\"“\"+\"–\"\n","    if len(exception)!=\"\":\n","        exception = list(exception)\n","        for i in exception:\n","            remove = remove.replace(i, \"\")\n","    translator = str.maketrans(remove , ' '* len(remove))\n","    text = text.translate(translator)\n","\n","    return re.sub(' +', ' ', text)\n","\n","def stringifyElement(text):\n","    return str(text)\n","\n","def transformContraction(text):\n","  \n","    CONTRACTIONS_DICT = { \n","        \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n","        \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n","        \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n","        \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n","        \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n","        \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n","        \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n","        \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n","        \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n","        \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n","        \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n","        \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n","        \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n","        \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n","        \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n","        \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n","        \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n","        \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n","        \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n","        \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n","        \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n","        \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n","        \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n","        \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n","        \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n","        \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n","        \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n","        \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n","        \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n","        \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n","        \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n","        \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n","        \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n","        \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n","        \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n","        \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n","        \"you've\": \"you have\"\n","    }\n","    for key in CONTRACTIONS_DICT.keys():\n","        text = text.lower().replace(key, CONTRACTIONS_DICT[key])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_7LLAE8k04S","outputId":"3aa3894d-9fc2-4f13-9f76-c1182561bb19"},"source":["!gdown --id 1aO2_CVo6aHuJNgkA5obJlkC6o3jlkxAN"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1aO2_CVo6aHuJNgkA5obJlkC6o3jlkxAN\n","To: /content/MRDA.zip\n","\r  0% 0.00/1.35M [00:00<?, ?B/s]\r100% 1.35M/1.35M [00:00<00:00, 90.0MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73Pky7mAkE4m","outputId":"bbf0dd17-805e-49ad-ae50-3d9e6a82f063"},"source":["!unzip /content/MRDA.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/MRDA.zip\n","  inflating: test.csv                \n","  inflating: train.csv               \n","  inflating: val.csv                 \n"]}]},{"cell_type":"code","metadata":{"id":"m2M-rFu_kP3R"},"source":["import pandas as pd \n","\n","# Đọc dữ liệu\n","train = pd.read_csv(\"train.csv\")\n","valid = pd.read_csv(\"val.csv\")\n","test = pd.read_csv(\"test.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXod2FoQlJCP","outputId":"f7e15972-6f16-4fa0-b8fe-893255e8c965"},"source":["list(train[\"Utterance_Text\"].values)[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['okay we are on.',\n"," 'so i think this is going to be a pretty short meeting because i have four agenda items.',\n"," 'three of them were requested by jane who is not going to be at the meeting today.',\n"," 'tsk.',\n"," 'so the uh first was transcription status.',\n"," 'does anyone besides jane know what the transcription status is?',\n"," 'um sort of i do peripherally.',\n"," 'um well first of all with i b m i got a note from brian yesterday saying that they finally made the tape for the thing that we sent them a week or week and a half ago.',\n"," 'is that english?',\n"," 'that is our system.']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"WR_H0JChMkAl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbb50456-c79a-4d86-841c-5b730f86233d"},"source":["train[\"Basic_DA_Tag\"].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    S\n","1    S\n","2    S\n","3    B\n","4    S\n","Name: Basic_DA_Tag, dtype: object"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"jFrchNfkM8-p"},"source":["# Chuyển sang dữ liệu string\n","train[\"Utterance_Text\"] = train[\"Utterance_Text\"].apply(stringifyElement)\n","valid[\"Utterance_Text\"] = valid[\"Utterance_Text\"].apply(stringifyElement)\n","test[\"Utterance_Text\"]  = test[\"Utterance_Text\"].apply(stringifyElement)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMxwEYVnhtOD"},"source":["# Chuyển về ký tự thường\n","train[\"Utterance_Text\"] = train[\"Utterance_Text\"].apply(lower_text)\n","valid[\"Utterance_Text\"] = valid[\"Utterance_Text\"].apply(lower_text)\n","test[\"Utterance_Text\"]  = test[\"Utterance_Text\"].apply(lower_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3Fh-F6EtZz4"},"source":["# Biến đổi rút gọn cụm từ rút gọn (can't => can not)\n","train[\"Utterance_Text\"] = train[\"Utterance_Text\"].apply(transformContraction)\n","valid[\"Utterance_Text\"] = valid[\"Utterance_Text\"].apply(transformContraction)\n","test[\"Utterance_Text\"] = test[\"Utterance_Text\"].apply(transformContraction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukFksQ-LP9MR"},"source":["# Xoá các ký tự đặc biệt, đấu câu chỉ giữ [.,?!]\n","train[\"Utterance_Text\"] = train[\"Utterance_Text\"].apply(seperatePunct)\n","valid[\"Utterance_Text\"] = valid[\"Utterance_Text\"].apply(seperatePunct)\n","test[\"Utterance_Text\"]  = test[\"Utterance_Text\"].apply(seperatePunct)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsfcVSU1RIJF"},"source":["# Chuyển hoá lại khoảng trống\n","train[\"Utterance_Text\"] = train[\"Utterance_Text\"].apply(normalizeWhitespace)\n","valid[\"Utterance_Text\"] = valid[\"Utterance_Text\"].apply(normalizeWhitespace)\n","test[\"Utterance_Text\"] = test[\"Utterance_Text\"].apply(normalizeWhitespace)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuT5aCxJ4_df"},"source":["#Khởi tạo tag sang id\n","tags = [\"S\", \"B\", \"D\", \"F\", \"Q\"]\n","from future.utils import iteritems\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","idx2tag = {v: k for k, v in iteritems(tag2idx)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDk1v7Kn4cGP"},"source":["train[\"Basic_DA_Tag\"] = train[\"Basic_DA_Tag\"].apply(lambda x : tag2idx[x])\n","valid[\"Basic_DA_Tag\"] = valid[\"Basic_DA_Tag\"].apply(lambda x : tag2idx[x])\n","test[\"Basic_DA_Tag\"] = test[\"Basic_DA_Tag\"].apply(lambda x : tag2idx[x])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ar_NwXd14ZoZ","colab":{"base_uri":"https://localhost:8080/","height":194},"outputId":"01ffdf23-67ad-48fe-ba2d-29218bb882cb"},"source":["train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>num</th>\n","      <th>Utterance_Text</th>\n","      <th>Basic_DA_Tag</th>\n","      <th>General_DA_Tag</th>\n","      <th>Full_DA_Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>okay we are on.</td>\n","      <td>0</td>\n","      <td>s</td>\n","      <td>s</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>so i think this is going to be a pretty short ...</td>\n","      <td>0</td>\n","      <td>s</td>\n","      <td>t</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>three of them were requested by jane who is no...</td>\n","      <td>0</td>\n","      <td>s</td>\n","      <td>s</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>tsk.</td>\n","      <td>1</td>\n","      <td>b</td>\n","      <td>b</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>so the uh first was transcription status.</td>\n","      <td>0</td>\n","      <td>h</td>\n","      <td>h</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   num  ... Full_DA_Tag\n","0    1  ...           s\n","1    1  ...           t\n","2    1  ...           s\n","3    1  ...           b\n","4    1  ...           h\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2R7j1WMxlToq","outputId":"2ab0a168-8620-42c3-bc62-b7a500a8e60e"},"source":["list(train[\"Utterance_Text\"].values)[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['okay we are on.',\n"," 'so i think this is going to be a pretty short meeting because i have four agenda items.',\n"," 'three of them were requested by jane who is not going to be at the meeting today.',\n"," 'tsk.',\n"," 'so the uh first was transcription status.',\n"," 'does anyone besides jane know what the transcription status is?',\n"," 'um sort of i do peripherally.',\n"," 'um well first of all with i b m i got a note from brian yesterday saying that they finally made the tape for the thing that we sent them a week or week and a half ago.',\n"," 'is that english?',\n"," 'that is our system.']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"HFfCqce2ovBc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhWBqbaQou3h"},"source":["### 1.2 EDA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbbpWOVknhL4","outputId":"74f3d795-689f-4e65-a68c-2d55efabb617"},"source":["max([len(c.split()) for c in train[\"Utterance_Text\"].values])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["84"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"RNYkERAwlc99","outputId":"3c12ef14-471e-42fc-cc56-2085aa94e52e"},"source":["import matplotlib.pyplot as plt\n","plt.hist([len(c.split()) for c in train[\"Utterance_Text\"].values], bins=50, range=(0,50))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0.0000e+00, 2.3992e+04, 5.5890e+03, 4.0090e+03, 3.8890e+03,\n","        3.5440e+03, 3.5610e+03, 3.3680e+03, 3.0840e+03, 2.7400e+03,\n","        2.6030e+03, 2.2390e+03, 2.0080e+03, 1.6740e+03, 1.6010e+03,\n","        1.4430e+03, 1.1860e+03, 1.0480e+03, 9.7200e+02, 8.7000e+02,\n","        7.0000e+02, 6.4600e+02, 5.3100e+02, 5.1000e+02, 4.1300e+02,\n","        3.6400e+02, 3.3200e+02, 2.8900e+02, 2.3400e+02, 2.0300e+02,\n","        1.9400e+02, 1.5200e+02, 1.3900e+02, 1.1800e+02, 1.0700e+02,\n","        9.0000e+01, 7.9000e+01, 6.9000e+01, 6.1000e+01, 6.3000e+01,\n","        3.6000e+01, 3.7000e+01, 4.0000e+01, 3.5000e+01, 1.8000e+01,\n","        2.2000e+01, 2.2000e+01, 1.7000e+01, 1.3000e+01, 2.3000e+01]),\n"," array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n","        13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n","        26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n","        39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50.]),\n"," <a list of 50 Patch objects>)"]},"metadata":{},"execution_count":20},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAD7CAYAAACIYvgKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQoElEQVR4nO3df6zddX3H8edrLTjjj7TAtenasqJ2WTozq95AF/wDMZaCZsXEENgmjSHWxJJg4jKr/9ShJJhM3UgcS5WGkiiVKIxG62rXkbj9AfZWECjIeocQ2hRaLYjGBFN874/zufOs3tt7e3/3nOcjOTnf7/v7+X7P55Oe9nW+n+/3nKaqkCT1tz+Y6w5IkuaeYSBJMgwkSYaBJAnDQJKEYSBJYgJhkGRFkgeSPJHkYJKbWv2zSY4keaQ9rura59NJhpM8leSKrvr6VhtOsqWrflGSh1r9m0nOne6BSpLGlvG+Z5BkKbC0qn6U5A3AAeBq4BrgV1X1D6e0Xw3cDVwM/BHw78CftM3/DbwPOAzsB66rqieS3APcW1U7k/wL8OOqun26BilJOr2F4zWoqqPA0bb8yyRPAstOs8sGYGdVvQL8NMkwnWAAGK6qpwGS7AQ2tONdDvxVa7MD+Cxw2jC44IILauXKleN1X5LU5cCBAz+rqoFT6+OGQbckK4F3AA8BlwI3JrkeGAI+WVUv0gmKB7t2O8zvwuO5U+qXAOcDL1XVyVHaj2nlypUMDQ2dSfclqe8leXa0+oQvICd5PfBt4BNV9TKdT+5vAdbQOXP44jT0c7w+bEoylGTo+PHjM/1yktQ3JhQGSc6hEwRfr6p7Aarqhap6tap+C3yV300FHQFWdO2+vNXGqv8cWJRk4Sn131NV26pqsKoGBwZ+7yxHkjRJE7mbKMAdwJNV9aWu+tKuZh8EHm/Lu4Brk7wmyUXAKuCHdC4Yr2p3Dp0LXAvsqs4V7AeAD7X9NwL3T21YkqQzMZFrBpcCHwYeS/JIq30GuC7JGqCAZ4CPAVTVwXZ30BPASWBzVb0KkORGYA+wANheVQfb8T4F7EzyeeBhOuEjSZol495aOl8NDg6WF5Al6cwkOVBVg6fW/QayJMkwkCQZBpIkDANJEmf4DeRet3LLd0etP3Pr+2e5J5I0uzwzkCQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxgTBIsiLJA0meSHIwyU2tfl6SvUkOtefFrZ4ktyUZTvJoknd2HWtja38oycau+ruSPNb2uS1JZmKwkqTRTeTM4CTwyapaDawFNidZDWwB9lXVKmBfWwe4EljVHpuA26ETHsBW4BLgYmDrSIC0Nh/t2m/91IcmSZqoccOgqo5W1Y/a8i+BJ4FlwAZgR2u2A7i6LW8A7qqOB4FFSZYCVwB7q+pEVb0I7AXWt21vrKoHq6qAu7qOJUmaBWd0zSDJSuAdwEPAkqo62jY9Dyxpy8uA57p2O9xqp6sfHqUuSZolEw6DJK8Hvg18oqpe7t7WPtHXNPdttD5sSjKUZOj48eMz/XKS1DcmFAZJzqETBF+vqntb+YU2xUN7PtbqR4AVXbsvb7XT1ZePUv89VbWtqgaranBgYGAiXZckTcBE7iYKcAfwZFV9qWvTLmDkjqCNwP1d9evbXUVrgV+06aQ9wLoki9uF43XAnrbt5SRr22td33UsSdIsWDiBNpcCHwYeS/JIq30GuBW4J8kNwLPANW3bbuAqYBj4NfARgKo6keRzwP7W7uaqOtGWPw7cCbwW+F57SJJmybhhUFX/BYx13/97R2lfwOYxjrUd2D5KfQh423h9kSTNDL+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIkJhEGS7UmOJXm8q/bZJEeSPNIeV3Vt+3SS4SRPJbmiq76+1YaTbOmqX5TkoVb/ZpJzp3OAkqTxTeTM4E5g/Sj1L1fVmvbYDZBkNXAt8Gdtn39OsiDJAuArwJXAauC61hbgC+1YbwVeBG6YyoAkSWdu3DCoqh8AJyZ4vA3Azqp6pap+CgwDF7fHcFU9XVW/AXYCG5IEuBz4Vtt/B3D1GY5BkjRFU7lmcGOSR9s00uJWWwY819XmcKuNVT8feKmqTp5SH1WSTUmGkgwdP358Cl2XJHWbbBjcDrwFWAMcBb44bT06jaraVlWDVTU4MDAwGy8pSX1h4WR2qqoXRpaTfBX4Tls9Aqzoarq81Rij/nNgUZKF7eygu70kaZZM6swgydKu1Q8CI3ca7QKuTfKaJBcBq4AfAvuBVe3OoXPpXGTeVVUFPAB8qO2/Ebh/Mn2SJE3euGcGSe4GLgMuSHIY2ApclmQNUMAzwMcAqupgknuAJ4CTwOaqerUd50ZgD7AA2F5VB9tLfArYmeTzwMPAHdM2OknShIwbBlV13SjlMf/BrqpbgFtGqe8Gdo9Sf5rO3UaSpDniN5AlSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliAmGQZHuSY0ke76qdl2RvkkPteXGrJ8ltSYaTPJrknV37bGztDyXZ2FV/V5LH2j63Jcl0D1KSdHoTOTO4E1h/Sm0LsK+qVgH72jrAlcCq9tgE3A6d8AC2ApcAFwNbRwKktflo136nvpYkaYaNGwZV9QPgxCnlDcCOtrwDuLqrfld1PAgsSrIUuALYW1UnqupFYC+wvm17Y1U9WFUF3NV1LEnSLJnsNYMlVXW0LT8PLGnLy4DnutodbrXT1Q+PUh9Vkk1JhpIMHT9+fJJdlySdasoXkNsn+pqGvkzktbZV1WBVDQ4MDMzGS0pSX5hsGLzQpnhoz8da/Qiwoqvd8lY7XX35KHVJ0iyabBjsAkbuCNoI3N9Vv77dVbQW+EWbTtoDrEuyuF04XgfsadteTrK23UV0fdexJEmzZOF4DZLcDVwGXJDkMJ27gm4F7klyA/AscE1rvhu4ChgGfg18BKCqTiT5HLC/tbu5qkYuSn+czh1LrwW+1x6SpFk0bhhU1XVjbHrvKG0L2DzGcbYD20epDwFvG68fkqSZ4zeQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJIELJzrDpwNVm757qj1Z259/yz3RJJmxpTODJI8k+SxJI8kGWq185LsTXKoPS9u9SS5LclwkkeTvLPrOBtb+0NJNk5tSJKkMzUd00Tvqao1VTXY1rcA+6pqFbCvrQNcCaxqj03A7dAJD2ArcAlwMbB1JEAkSbNjJq4ZbAB2tOUdwNVd9buq40FgUZKlwBXA3qo6UVUvAnuB9TPQL0nSGKYaBgV8P8mBJJtabUlVHW3LzwNL2vIy4LmufQ+32lh1SdIsmeoF5HdX1ZEkbwL2JvlJ98aqqiQ1xdf4Py1wNgFceOGF03VYSep7UzozqKoj7fkYcB+dOf8X2vQP7flYa34EWNG1+/JWG6s+2uttq6rBqhocGBiYStclSV0mHQZJXpfkDSPLwDrgcWAXMHJH0Ebg/ra8C7i+3VW0FvhFm07aA6xLsrhdOF7XapKkWTKVaaIlwH1JRo7zjar6tyT7gXuS3AA8C1zT2u8GrgKGgV8DHwGoqhNJPgfsb+1urqoTU+iXJOkMTToMqupp4O2j1H8OvHeUegGbxzjWdmD7ZPsiSZoaf45CkmQYSJIMA0kS/lDdlPgDdpJ6hWEwAwwJSWcbp4kkSYaBJMlpolk11vTR6Ti1JGk2eGYgSfLMYL7zYrSk2eCZgSTJM4OzlWcMkqaTZwaSJMNAkuQ0Uc9x+kjSZBgGfcKQkHQ6ThNJkgwDSZLTRH3P6SNJ4JmBJAnPDDQGzxik/uKZgSTJMJAkOU2kM+T0kdSbDANNC0NCOrsZBppRhoR0dvCagSTJMJAkOU2kOeL0kTS/GAaaV8YKidMxQKSpMwx01vMsQ5o6rxlIkjwzUO860yknzyTUzwwDqTE81M/mTRgkWQ/8E7AA+FpV3TrHXZJOy2sV6iXzIgySLAC+ArwPOAzsT7Krqp6Y255JZ24yd0SNxlDRbJoXYQBcDAxX1dMASXYCGwDDQH1rukLldAwcjZgvYbAMeK5r/TBwyRz1ReobsxE4Z7t+Ccz5EgYTkmQTsKmt/irJU5M81AXAz6anV2cNx9wf+m3MMz7efGEmjz4pUx3zH49WnC9hcARY0bW+vNX+n6raBmyb6oslGaqqwake52zimPtDv42538YLMzfm+fKls/3AqiQXJTkXuBbYNcd9kqS+MS/ODKrqZJIbgT10bi3dXlUH57hbktQ35kUYAFTVbmD3LL3clKeazkKOuT/025j7bbwwQ2NOVc3EcSVJZ5H5cs1AkjSH+ioMkqxP8lSS4SRb5ro/MyXJ9iTHkjzeVTsvyd4kh9rz4rns43RKsiLJA0meSHIwyU2t3stj/sMkP0zy4zbmv2/1i5I81N7j32w3ZPSUJAuSPJzkO229p8ec5JkkjyV5JMlQq037e7tvwqDrJy+uBFYD1yVZPbe9mjF3AutPqW0B9lXVKmBfW+8VJ4FPVtVqYC2wuf3Z9vKYXwEur6q3A2uA9UnWAl8AvlxVbwVeBG6Ywz7OlJuAJ7vW+2HM76mqNV23lE77e7tvwoCun7yoqt8AIz950XOq6gfAiVPKG4AdbXkHcPWsdmoGVdXRqvpRW/4lnX8oltHbY66q+lVbPac9Crgc+Far99SYAZIsB94PfK2thx4f8xim/b3dT2Ew2k9eLJujvsyFJVV1tC0/DyyZy87MlCQrgXcAD9HjY27TJY8Ax4C9wP8AL1XVydakF9/j/wj8HfDbtn4+vT/mAr6f5ED7FQaYgff2vLm1VLOnqipJz91GluT1wLeBT1TVy50PjR29OOaqehVYk2QRcB/wp3PcpRmV5APAsao6kOSyue7PLHp3VR1J8iZgb5KfdG+crvd2P50ZTOgnL3rYC0mWArTnY3Pcn2mV5Bw6QfD1qrq3lXt6zCOq6iXgAeAvgEVJRj7k9dp7/FLgL5M8Q2ea93I6/wdKL4+ZqjrSno/RCf2LmYH3dj+FQb//5MUuYGNb3gjcP4d9mVZt3vgO4Mmq+lLXpl4e80A7IyDJa+n8XyBP0gmFD7VmPTXmqvp0VS2vqpV0/v7+R1X9NT085iSvS/KGkWVgHfA4M/De7qsvnSW5is6c48hPXtwyx12aEUnuBi6j8+uGLwBbgX8F7gEuBJ4FrqmqUy8yn5WSvBv4T+AxfjeX/Bk61w16dcx/TufC4QI6H+ruqaqbk7yZzqfm84CHgb+pqlfmrqczo00T/W1VfaCXx9zGdl9bXQh8o6puSXI+0/ze7qswkCSNrp+miSRJYzAMJEmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEvC/hGwiTeLrHrUAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"gwfLYyZLn7rI"},"source":["* Max từ trong câu là 84 từ\n","* Những câu có 0 từ, 1 từ khá nhiều"]},{"cell_type":"code","metadata":{"id":"ga8OjEc5nM-k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_AnwQvboys5"},"source":["### 1.3 Data Loader"]},{"cell_type":"code","metadata":{"id":"qVmijMZQSA-L"},"source":["import numpy as np\n","def split_conversation(df):\n","    x_data = []\n","    y_data = []\n","    for num in np.unique(df[\"num\"]):\n","        indx = (df[\"num\"] == num)\n","        x_data.append(list(df[indx][\"Utterance_Text\"].values))\n","        y_data.append(list(df[indx][\"Basic_DA_Tag\"].values))\n","    return x_data, y_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpaoZCXmrnqe"},"source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn\n","\n","class DADataset(Dataset):\n","    \n","    def __init__(self, tokenizer, text, label, max_len=512):\n","        \n","        self.text = text\n","        self.label = label\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __len__(self):\n","        return len(self.text)\n","    \n","    def __getitem__(self, index):\n","        \n","        sentences = self.text[index]\n","        labels = self.label[index]\n","        \n","        # Tokenize all of the sentences and map the tokens to thier word IDs.\n","        input_ids = []\n","        attention_masks = []\n","\n","        # For every sentence...\n","        for sent in sentences:\n","            # `encode_plus` will:\n","            #   (1) Tokenize the sentence.\n","            #   (2) Prepend the `[CLS]` token to the start.\n","            #   (3) Append the `[SEP]` token to the end.\n","            #   (4) Map tokens to their IDs.\n","            #   (5) Pad or truncate the sentence to `max_length`\n","            #   (6) Create attention masks for [PAD] tokens.\n","            encoded_dict = tokenizer.encode_plus(\n","                                sent,                      # Sentence to encode.\n","                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                                max_length = 64,           # Pad & truncate all sentences.\n","                                pad_to_max_length = True,\n","                                return_attention_mask = True,   # Construct attn. masks.\n","                                return_tensors = 'pt',     # Return pytorch tensors.\n","                        )\n","            \n","            # Add the encoded sentence to the list.    \n","            input_ids.append(encoded_dict['input_ids'])\n","            \n","            # And its attention mask (simply differentiates padding from non-padding).\n","            attention_masks.append(encoded_dict['attention_mask'])\n","\n","        # Convert the lists into tensors.\n","        input_ids = torch.cat(input_ids, dim=0)\n","        attention_masks = torch.cat(attention_masks, dim=0)\n","        labels = torch.tensor(labels)\n","        \n","        return {\n","            \"input_ids\":input_ids,\n","            \"attention_mask\":attention_masks,\n","            \"label\":labels,\n","        }\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kij2CGlhpCJY"},"source":["!pip install transformers\n","from transformers import BertTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVGbixsdpBMC","outputId":"efad0934-3c64-48ab-9a18-de30e96a43c1"},"source":["# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]}]},{"cell_type":"code","metadata":{"id":"fdjVRhvkuEuJ"},"source":["X_train, y_train = split_conversation(train)\n","train_dataset = DADataset(tokenizer=tokenizer, text = X_train, label = y_train, max_len=512)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXRWmDBPy_2f"},"source":["X_valid, y_valid = split_conversation(valid)\n","val_dataset = DADataset(tokenizer=tokenizer, text = X_valid, label = y_valid, max_len=512)\n","val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iSvYVMNzOgo"},"source":["X_test, y_test = split_conversation(test)\n","test_dataset = DADataset(tokenizer=tokenizer, text = X_test, label = y_test, max_len=512)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, drop_last=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjMsLdtQpQsm","outputId":"7c5fbdde-c0d1-444c-80a1-1adbf3279554"},"source":["len(train_loader), len(val_loader), len(test_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(51, 12, 12)"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"2jh6skSBppyw"},"source":["* Train: 51 hộp thoại\n","* Val: 12 hộp thoại\n","* Test: 12 hộp thoại"]},{"cell_type":"code","metadata":{"id":"2Bw-03ZzpWe7"},"source":["số câu x Seqlength"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1Fj1VO6pYee"},"source":["## 2.Modeling"]},{"cell_type":"code","metadata":{"id":"Kzm2y9dzqfHL"},"source":["import torch.nn as nn\n","import torch\n","from transformers import AutoConfig, AutoModel, AutoTokenizer\n","\n","\n","class Word_RNN(nn.Module):\n","    \n","    def __init__(self, model_name=\"bert-base-uncased\", hidden_size=768, bidirectional=True, num_layers=1):\n","        super(Word_RNN, self).__init__()\n","        \n","        \n","        # embedding layer is replaced by pretrained roberta's embedding\n","        self.base = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name)\n","        \n","        # freeze the model parameters\n","        for param in self.base.parameters():\n","            param.requires_grad = False\n","        \n","        self.rnn = nn.LSTM(\n","            input_size=hidden_size, \n","            hidden_size=hidden_size, \n","            num_layers=num_layers, \n","            bidirectional=bidirectional,\n","            batch_first=True\n","        )\n","    \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","            x.shape = [batch_size, seq_len]\n","        \"\"\"\n","        \n","        input_ids = input_ids.squeeze(0)\n","        attention_mask = attention_mask.squeeze(0)\n","        hidden_states = self.base(input_ids, attention_mask)[0] # hidden_states.shape = [batch, max_len, hidden_size]\n","        \n","        _,(outputs, _) = self.rnn(hidden_states)\n","                \n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR5deUZGxghI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9nJl70wIatu"},"source":["class Pooling_HBLSTM(nn.Module):\n","    \n","    def __init__(self, hidden_size=768, num_classes=43, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n","        \n","        super(Pooling_HBLSTM, self).__init__()\n","        \n","        self.in_features = 2*hidden_size\n","        \n","        self.device = device\n","        \n","        # Word_RNN encoder model\n","        self.word_RNN = Word_RNN(\"bert-base-uncased\", hidden_size=hidden_size).to(device)\n","\n","        # Conversation_RNN encoder model\n","        self.conversation_RNN = nn.LSTM(\n","            input_size=self.in_features, \n","            hidden_size=hidden_size, \n","            num_layers=1, \n","            bidirectional=True,\n","            batch_first=True\n","        ).to(device)\n","        \n","        # classifier on top of feature extractor\n","        self.classifier = nn.Sequential(*[\n","            nn.Linear(in_features=self.in_features, out_features=256),\n","            nn.LeakyReLU(),\n","            nn.Linear(in_features=256, out_features=128),\n","            nn.LeakyReLU(),\n","            nn.Linear(in_features=128, out_features=num_classes)\n","        ]).to(device)\n","    \n","    def forward(self, inputs):\n","        \"\"\"\n","            inputs.shape = [batch, seq_len, hidden_size]\n","        \"\"\"\n","        \n","        #outputs 2 x số câu x hidden_size\n","        x = self.word_RNN(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n","\n","        x = torch.transpose(x, 1, 0) # số câu x 2 x hidden_size\n","        x = x.reshape(x.shape[0], -1) # số câu x 2hidden_size\n","\n","        x = x.unsqueeze(0) # 1 x số câu x 2hidden_size\n","        x,_ = self.conversation_RNN(x)  # 1 x số câu x 2hidden_size\n","\n","        x = x.squeeze(0) #sốcâu x 2hidden_size\n","        \n","        logits = self.classifier(x)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0IH1LDvBjxf"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqrIeyGLtYJd","colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["9531dab51e2642839f1554b484e47976","9b8354c252294f9d8653a1eb2ec9d3fe","a3a312d3156a4bd094474fa17c1a65fe","99fb8c1ff81244f3858cbf7ef5cbcaaa","a46cdc7b288941b3adb5807e931d059d","956cc5fbf44a4a6fab3bd40aba508c55","c452e8c4388949d7a19190c4540e91a9","db03e5467ce94359b15c670c540bb078","97c5e0e474454608bb80e9c854110a8c","c1730b9373d343abb5a20abecc4d2d71","efbe4c866d5a4bc39b3d40d936ad5e4c"]},"outputId":"f62ed4a7-ba7e-49bd-eb66-e5c86d2b10ce"},"source":["model = Pooling_HBLSTM(hidden_size=768, num_classes=5, device=device).to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9531dab51e2642839f1554b484e47976","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"5m_iEANerOxu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmmecHn6GCLQ"},"source":["def evaluate(model, data_loader):\n","    accuracies = []\n","    losses = []\n","    model.eval()\n","    with torch.no_grad():\n","        for x in data_loader:\n","            # Forward pass\n","            targets = x[\"label\"].squeeze(0).to(device)\n","            outputs = model(x)\n","            loss = losser(outputs, targets)\n","\n","            accuracy = (outputs.argmax(dim=-1) == targets).type(torch.float32).mean().item()\n","            accuracies.append(accuracy)\n","            losses.append(loss.item())\n","    \n","    return np.mean(losses), np.mean(accuracies)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TILZZCh6zX1G"},"source":["from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFsSb97IKpMd"},"source":["scaler = torch.cuda.amp.GradScaler()\n","train_losses = []\n","train_accs = []\n","test_losses = []\n","test_accs = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSSwCb8rsqGx"},"source":["# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by 2x every 10 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=10,\n","                                               gamma=0.5)\n","losser = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kd0ejOLftHzY"},"source":["# Xem có bao nhiêu params => 133_526_789\n","# sum(p.numel() for p in model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQtT7fyAzsFp","outputId":"79619997-6d91-4912-e1da-ba17d7416103"},"source":["n_batch = len(train_loader)\n","max_acc = 0\n","\n","for epoch in range(2):\n","    model.train()\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        tepoch.set_description(f\"Epoch {epoch + 1}\")\n","        for batch_idx, x in enumerate(tepoch):\n","\n","            targets = x[\"label\"].squeeze(0).to(device)\n","\n","            #forward\n","            with torch.cuda.amp.autocast():\n","                logits = model(x)\n","                loss = losser(logits, targets)\n","\n","            # Backward\n","            optimizer.zero_grad()\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","\n","            train_accs.append((logits.argmax(dim=-1) == targets).type(torch.float32).mean().item())\n","            train_losses.append(loss.item())\n","\n","            tepoch.set_postfix(loss=train_losses[-1], acc=train_accs[-1])\n","\n","            if batch_idx >= n_batch - 1:\n","                val_loss, val_acc =  evaluate(model, test_loader)\n","                test_losses.append(val_loss)\n","                test_accs.append(val_acc)\n","                tepoch.set_postfix(loss=np.mean(train_losses), acc=np.mean(train_accs), val_loss=val_loss, val_acc=val_acc)\n","\n","    if max_acc < val_acc:\n","        max_acc = val_acc\n","        print(f\"Save at epoch={epoch+1} with lr={lr_scheduler.get_last_lr()} and loss={max_acc}\")\n","        torch.save(model.state_dict(), \"/content/Pooling_HBLSTM_MRDA.pth\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 51/51 [04:24<00:00,  5.19s/batch, acc=0.596, loss=1.22, val_acc=0.574, val_loss=1.03]\n"]},{"output_type":"stream","name":"stdout","text":["Save at epoch=1 with lr=[0.0001] and loss=0.5737020572026571\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 51/51 [04:24<00:00,  5.19s/batch, acc=0.649, loss=0.999, val_acc=0.764, val_loss=0.587]\n"]},{"output_type":"stream","name":"stdout","text":["Save at epoch=2 with lr=[0.0001] and loss=0.7641851554314295\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UY8MIMoY-oIK","outputId":"3aa1934f-0ee2-4cf4-da1f-99894c9edcec"},"source":["evaluate(model, test_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.5866002018253008, 0.7641851554314295)"]},"metadata":{},"execution_count":85}]}]}